{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-edde57d30296>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-edde57d30296>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    from pyspark.sqlimport SparkSession\u001b[0m\n\u001b[1;37m                                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark# only run after findspark.init()\n",
    "from pyspark.sqlimport SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df= spark.sql('''select 'spark' as hello ''')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|hello|\n",
      "+-----+\n",
      "|spark|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df = spark.sql('''select 'spark' as hello ''')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "spark = SparkSession.builder.master(\"local[2]\").appName(\"word count\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read all files of type csv in the location mentioned, when setting \"Header\" to true, means the first line of files name columns and are not included in data. All types are assumed to be string.\n",
    "#\"inferSchema\" automatically infer column types. It requires one extra pass over the data and is false by default. \n",
    "#Coalesce it is recommended to use coalesce while reducing no. of partitions, it only reads the rows with description that are not null.\n",
    "sales = spark.read.format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".load(\"Spark-The-Definitive-Guide-master/data/retail-data/by-day/*.csv\")\\\n",
    ".coalesce(5)\\\n",
    ".where(\"Description IS NOT NULL\")\n",
    "#this reads the json file in this specified folder location.\n",
    "simpleDF = spark.read.json(\"Spark-The-Definitive-Guide-master/data/simple-ml\")\n",
    "#this reads the parquet file in this specified folder location.\n",
    "scaleDF = spark.read.parquet(\"Spark-The-Definitive-Guide-master/data/simple-ml-scaling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-----------------------------------+\n",
      "| id|      features|StandardScaler_c93a10577b2d__output|\n",
      "+---+--------------+-----------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|               [1.19522860933439...|\n",
      "|  1| [2.0,1.1,1.0]|               [2.39045721866878...|\n",
      "|  0|[1.0,0.1,-1.0]|               [1.19522860933439...|\n",
      "|  1| [2.0,1.1,1.0]|               [2.39045721866878...|\n",
      "|  1|[3.0,10.1,3.0]|               [3.58568582800318...|\n",
      "+---+--------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "#StandardScaler standarizes a feature by subtracting the mean and then scaling to unit variance. Unit variance means dividing all the values by the standard deviation.\n",
    "sScaler = StandardScaler().setInputCol(\"features\")\n",
    "# Compute summary statistics by fitting the StandardScaler and Normalize each feature to have unit standard deviation.\n",
    "sScaler.fit(scaleDF).transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+---------------------------------+\n",
      "| id|      features|MinMaxScaler_a452a61c2e5c__output|\n",
      "+---+--------------+---------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|                    [5.0,5.0,5.0]|\n",
      "|  1| [2.0,1.1,1.0]|                    [7.5,5.5,7.5]|\n",
      "|  0|[1.0,0.1,-1.0]|                    [5.0,5.0,5.0]|\n",
      "|  1| [2.0,1.1,1.0]|                    [7.5,5.5,7.5]|\n",
      "|  1|[3.0,10.1,3.0]|                 [10.0,10.0,10.0]|\n",
      "+---+--------------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "#Transform features by scaling each feature to a given range, which is from 5 to 10 here.\n",
    "#replaces the values below 1.0 and 1.0 in features to 5.0 in MinMaxScaler new column. And so on rescale each value in features\n",
    "#column to fit the range.\n",
    "minMax = MinMaxScaler().setMin(5).setMax(10).setInputCol(\"features\")\n",
    "fittedminMax = minMax.fit(scaleDF)\n",
    "#Scale features of fittedminMax according to feature_range.\n",
    "# rescale each feature to range [min, max].\n",
    "fittedminMax.transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-------------------------------+\n",
      "| id|      features|Normalizer_ebd61adb20d8__output|\n",
      "+---+--------------+-------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|           [0.47619047619047...|\n",
      "|  1| [2.0,1.1,1.0]|           [0.48780487804878...|\n",
      "|  0|[1.0,0.1,-1.0]|           [0.47619047619047...|\n",
      "|  1| [2.0,1.1,1.0]|           [0.48780487804878...|\n",
      "|  1|[3.0,10.1,3.0]|           [0.18633540372670...|\n",
      "+---+--------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "#Normalizer is a Transformer which transforms a dataset of Vector rows, normalizing each feature to have unit norm. \n",
    "#It takes parameter p, which specifies the p-norm used for normalization. \n",
    "#This normalization can help standardize your input data and improve the behavior of learning algorithms.\n",
    "manhattanDistance = Normalizer().setP(1).setInputCol(\"features\")\n",
    "# Normalize each feature using norm.\n",
    "manhattanDistance.transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------+\n",
      "|color| lab|value1|            value2|labelInd|\n",
      "+-----+----+------+------------------+--------+\n",
      "|green|good|     1|14.386294994851129|     1.0|\n",
      "| blue| bad|     8|14.386294994851129|     0.0|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|\n",
      "|green|good|    15| 38.97187133755819|     1.0|\n",
      "|green|good|    12|14.386294994851129|     1.0|\n",
      "|green| bad|    16|14.386294994851129|     0.0|\n",
      "|  red|good|    35|14.386294994851129|     1.0|\n",
      "|  red| bad|     1| 38.97187133755819|     0.0|\n",
      "|  red| bad|     2|14.386294994851129|     0.0|\n",
      "|  red| bad|    16|14.386294994851129|     0.0|\n",
      "|  red|good|    45| 38.97187133755819|     1.0|\n",
      "|green|good|     1|14.386294994851129|     1.0|\n",
      "| blue| bad|     8|14.386294994851129|     0.0|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|\n",
      "|green|good|    15| 38.97187133755819|     1.0|\n",
      "|green|good|    12|14.386294994851129|     1.0|\n",
      "|green| bad|    16|14.386294994851129|     0.0|\n",
      "|  red|good|    35|14.386294994851129|     1.0|\n",
      "|  red| bad|     1| 38.97187133755819|     0.0|\n",
      "|  red| bad|     2|14.386294994851129|     0.0|\n",
      "+-----+----+------+------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "#StringIndexer encodes a string column of labels to a column of label indices.\n",
    "#in lab column he replaced each good value to 1.0 in labelInd and each bad value to be 0.0 in labelInd.\n",
    "#It shows the first 20 rows only.\n",
    "lblIndxr = StringIndexer().setInputCol(\"lab\").setOutputCol(\"labelInd\")\n",
    "idxRes = lblIndxr.fit(simpleDF).transform(simpleDF)\n",
    "idxRes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------+\n",
      "|color| lab|value1|            value2|valueInd|\n",
      "+-----+----+------+------------------+--------+\n",
      "|green|good|     1|14.386294994851129|     0.0|\n",
      "| blue| bad|     8|14.386294994851129|     7.0|\n",
      "| blue| bad|    12|14.386294994851129|     1.0|\n",
      "|green|good|    15| 38.97187133755819|     3.0|\n",
      "|green|good|    12|14.386294994851129|     1.0|\n",
      "|green| bad|    16|14.386294994851129|     2.0|\n",
      "|  red|good|    35|14.386294994851129|     5.0|\n",
      "|  red| bad|     1| 38.97187133755819|     0.0|\n",
      "|  red| bad|     2|14.386294994851129|     4.0|\n",
      "|  red| bad|    16|14.386294994851129|     2.0|\n",
      "|  red|good|    45| 38.97187133755819|     6.0|\n",
      "|green|good|     1|14.386294994851129|     0.0|\n",
      "| blue| bad|     8|14.386294994851129|     7.0|\n",
      "| blue| bad|    12|14.386294994851129|     1.0|\n",
      "|green|good|    15| 38.97187133755819|     3.0|\n",
      "|green|good|    12|14.386294994851129|     1.0|\n",
      "|green| bad|    16|14.386294994851129|     2.0|\n",
      "|  red|good|    35|14.386294994851129|     5.0|\n",
      "|  red| bad|     1| 38.97187133755819|     0.0|\n",
      "|  red| bad|     2|14.386294994851129|     4.0|\n",
      "+-----+----+------+------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#StringIndexer encodes a string column of value1 to a column of value indices.\n",
    "#here encodes value1 column to valueInd coumn.\n",
    "#If the input column is numeric, we cast it to string and index the string values. As value1 is a numeric column.\n",
    "valIndexer = StringIndexer().setInputCol(\"value1\").setOutputCol(\"valueInd\")\n",
    "valIndexer.fit(simpleDF).transform(simpleDF).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+----------------------------------+\n",
      "|color|colorInd|OneHotEncoder_0b6899e3c10b__output|\n",
      "+-----+--------+----------------------------------+\n",
      "|green|     1.0|                     (2,[1],[1.0])|\n",
      "| blue|     2.0|                         (2,[],[])|\n",
      "| blue|     2.0|                         (2,[],[])|\n",
      "|green|     1.0|                     (2,[1],[1.0])|\n",
      "|green|     1.0|                     (2,[1],[1.0])|\n",
      "|green|     1.0|                     (2,[1],[1.0])|\n",
      "|  red|     0.0|                     (2,[0],[1.0])|\n",
      "|  red|     0.0|                     (2,[0],[1.0])|\n",
      "|  red|     0.0|                     (2,[0],[1.0])|\n",
      "|  red|     0.0|                     (2,[0],[1.0])|\n",
      "|  red|     0.0|                     (2,[0],[1.0])|\n",
      "|green|     1.0|                     (2,[1],[1.0])|\n",
      "| blue|     2.0|                         (2,[],[])|\n",
      "| blue|     2.0|                         (2,[],[])|\n",
      "|green|     1.0|                     (2,[1],[1.0])|\n",
      "|green|     1.0|                     (2,[1],[1.0])|\n",
      "|green|     1.0|                     (2,[1],[1.0])|\n",
      "|  red|     0.0|                     (2,[0],[1.0])|\n",
      "|  red|     0.0|                     (2,[0],[1.0])|\n",
      "|  red|     0.0|                     (2,[0],[1.0])|\n",
      "+-----+--------+----------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "#First, StringIndexer encodes a string column of color to a column of color indices.\n",
    "#Then, One-hot encoding transforms the values in colorInd into a binary vector with at most a single one-value per row,\n",
    "#that indicates the input color index.\n",
    "#Since there are three values, the vector is of length 2\n",
    "lblIndxr = StringIndexer().setInputCol(\"color\").setOutputCol(\"colorInd\")\n",
    "colorLab = lblIndxr.fit(simpleDF).transform(simpleDF.select(\"color\"))\n",
    "ohe = OneHotEncoder().setInputCol(\"colorInd\")\n",
    "ohe.fit(colorLab).transform(colorLab).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------------------------------+\n",
      "|Description                        |DescOut                                   |\n",
      "+-----------------------------------+------------------------------------------+\n",
      "|RABBIT NIGHT LIGHT                 |[rabbit, night, light]                    |\n",
      "|DOUGHNUT LIP GLOSS                 |[doughnut, lip, gloss]                    |\n",
      "|12 MESSAGE CARDS WITH ENVELOPES    |[12, message, cards, with, envelopes]     |\n",
      "|BLUE HARMONICA IN BOX              |[blue, harmonica, in, box]                |\n",
      "|GUMBALL COAT RACK                  |[gumball, coat, rack]                     |\n",
      "|SKULLS  WATER TRANSFER TATTOOS     |[skulls, water, transfer, tattoos]        |\n",
      "|FELTCRAFT GIRL AMELIE KIT          |[feltcraft, girl, amelie, kit]            |\n",
      "|CAMOUFLAGE LED TORCH               |[camouflage, led, torch]                  |\n",
      "|WHITE SKULL HOT WATER BOTTLE       |[white, skull, hot, water, bottle]        |\n",
      "|ENGLISH ROSE HOT WATER BOTTLE      |[english, rose, hot, water, bottle]       |\n",
      "|HOT WATER BOTTLE KEEP CALM         |[hot, water, bottle, keep, calm]          |\n",
      "|SCOTTIE DOG HOT WATER BOTTLE       |[scottie, dog, hot, water, bottle]        |\n",
      "|ROSE CARAVAN DOORSTOP              |[rose, caravan, doorstop]                 |\n",
      "|GINGHAM HEART  DOORSTOP RED        |[gingham, heart, doorstop, red]           |\n",
      "|STORAGE TIN VINTAGE LEAF           |[storage, tin, vintage, leaf]             |\n",
      "|SET OF 4 KNICK KNACK TINS POPPIES  |[set, of, 4, knick, knack, tins, poppies] |\n",
      "|POPCORN HOLDER                     |[popcorn, holder]                         |\n",
      "|GROW A FLYTRAP OR SUNFLOWER IN TIN |[grow, a, flytrap, or, sunflower, in, tin]|\n",
      "|AIRLINE BAG VINTAGE WORLD CHAMPION |[airline, bag, vintage, world, champion]  |\n",
      "|AIRLINE BAG VINTAGE JET SET BROWN  |[airline, bag, vintage, jet, set, brown]  |\n",
      "+-----------------------------------+------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "#RegexTokenizer() takes as input Description column and returns as output DescOut.\n",
    "#This Function takes String from Description column and make the Capital letters to small, and put the whole string\n",
    "#into array with comma between each word and another.\n",
    "rt = RegexTokenizer()\\\n",
    ".setInputCol(\"Description\")\\\n",
    ".setOutputCol(\"DescOut\")\\\n",
    ".setPattern(\" \")\\\n",
    ".setToLowercase(True)\n",
    "rt.transform(sales.select(\"Description\")).show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fittedCV' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-113f23f414ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msales\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Description\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"CustomerId\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"CustomerId IS NOT NULL\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mprechi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfittedCV\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenized\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"CustomerId IS NOT NULL\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mchisq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mChiSqSelector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fittedCV' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import ChiSqSelector, Tokenizer\n",
    "tkn = Tokenizer().setInputCol(\"Description\").setOutputCol(\"DescOut\")\n",
    "tokenized = tkn\\\n",
    ".transform(sales.select(\"Description\", \"CustomerId\"))\\\n",
    ".where(\"CustomerId IS NOT NULL\")\n",
    "prechi = fittedCV.transform(tokenized)\\\n",
    ".where(\"CustomerId IS NOT NULL\")\n",
    "chisq = ChiSqSelector()\\\n",
    ".setFeaturesCol(\"countVec\")\\\n",
    ".setLabelCol(\"CustomerId\")\\\n",
    ".setNumTopFeatures(2)\n",
    "chisq.fit(prechi).transform(prechi)\\\n",
    ".drop(\"customerId\", \"Description\", \"DescOut\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
